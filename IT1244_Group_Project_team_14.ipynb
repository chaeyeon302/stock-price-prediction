{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500_data = []\n",
    "data = {}\n",
    "date = {}\n",
    "\n",
    "with open('sp500wiki.csv', 'r') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    for row in reader:\n",
    "        sp500_data.append(row)\n",
    "        data[row['Symbol']] = [] # create the empty list for each symbol(each company symbol is key, value is the associated data)\n",
    "    file.close()\n",
    "\n",
    "columnNames = ['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'News - All News Volume', 'News - Volume']\n",
    "\n",
    "with open('data.csv', 'r') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    for row in reader:\n",
    "        index = len(data[row['Symbol']]) # convert date to index\n",
    "        date[row['Date']] = index\n",
    "        row['Date'] = index\n",
    "        data_list = []\n",
    "        for col in columnNames:\n",
    "            data_list.append(row[col]) # do not add data with many '0' values\n",
    "        data[row['Symbol']].append(data_list) # key is symbol and value is the row with significant features\n",
    "    file.close()\n",
    "\n",
    "# remove rows with any empty values\n",
    "for symbol_data in data.values():\n",
    "    for row in symbol_data:\n",
    "        for col in columnNames:\n",
    "            if row[columnNames.index(col)] == '':\n",
    "                symbol_data.remove(row)\n",
    "                columnNames.remove(col)\n",
    "\n",
    "# calculate 5-day moving average values for 4 features\n",
    "def calculate_moving_average(feature, data):\n",
    "    for symbol_data in data.values():\n",
    "        for index in range(len(symbol_data)):\n",
    "            if index > 4:\n",
    "                values = []\n",
    "                i = 1\n",
    "                while i <= 5:\n",
    "                    values.append(float(symbol_data[index-i][columnNames.index(feature)]))\n",
    "                    i += 1\n",
    "                MV = sum(values)/5\n",
    "                symbol_data[index].append(MV)\n",
    "\n",
    "calculate_moving_average('Open', data)\n",
    "calculate_moving_average('High', data)\n",
    "calculate_moving_average('Low', data)\n",
    "calculate_moving_average('Close', data)\n",
    "\n",
    "# removing rows without 5-day moving average values\n",
    "for symbol_data in data.values():\n",
    "    del symbol_data[:5]\n",
    "\n",
    "# changing string to float\n",
    "for symbol_data in data.values():\n",
    "    for row in symbol_data:\n",
    "        for i in range(len(row)):\n",
    "            row[i] = float(row[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making X dataset and y dataset for specific symbol and 4 features(open, high, low, close)\n",
    "def createData(symbol, value, data):\n",
    "    symbolData = []\n",
    "    for symbols, symbol_data in data.items():\n",
    "        if symbols == symbol:\n",
    "            symbolData = symbol_data\n",
    "            break\n",
    "\n",
    "    xData = []\n",
    "    yData = []\n",
    "\n",
    "    itr = len(symbolData)\n",
    "    if value == 'Open':\n",
    "        index = 0\n",
    "    elif value == 'High':\n",
    "        index = 1\n",
    "    elif value == 'Low':\n",
    "        index = 2\n",
    "    elif value == 'Close':\n",
    "        index = 3\n",
    "\n",
    "    for i in range(itr-1, 0, -1):\n",
    "        # adding the 1-day lag value for all  featues except the 5-day moving average value of one of the 4 features(open, high, low, close)\n",
    "        xRow = symbolData[i - 1][0:8] + [symbolData[i][index - 4]]\n",
    "        # adding the current day value of one of the 4 features(open, high, low, close)\n",
    "        yVal = symbolData[i][index]\n",
    "        xData.append(xRow)\n",
    "        yData.append(yVal)\n",
    "\n",
    "    return(xData, yData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLRscaler = MinMaxScaler()\n",
    "\n",
    "def linear_regression(symbol, value, data):\n",
    "    model = LinearRegression()\n",
    "\n",
    "    xData, yData = createData(symbol, value, data)\n",
    "\n",
    "    # training data 0.8, testing data 0.2\n",
    "    split_range = lambda x: int(len(x) * 0.8)\n",
    "    x_train, x_test = xData[:split_range(xData)], xData[split_range(xData):]\n",
    "    y_train, y_test = yData[:split_range(yData)], yData[split_range(yData):]\n",
    "    \n",
    "    # Normalize the input data (x_train and x_test)\n",
    "    x_train = MLRscaler.fit_transform(x_train)\n",
    "    x_test = MLRscaler.transform(x_test)\n",
    "\n",
    "    # Reshape the y data for the scaler\n",
    "    y_train = np.array(y_train).reshape(-1, 1)\n",
    "    y_test = np.array(y_test).reshape(-1, 1)\n",
    "\n",
    "    # Normalize the output data (y_train and y_test)\n",
    "    y_train = MLRscaler.fit_transform(y_train)\n",
    "    y_test = MLRscaler.transform(y_test)\n",
    "\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "\n",
    "    \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    print(f'Root Mean Squared Error (RMSE): {rmse}')\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mlr_predict(symbol, desired_date, value, model):\n",
    "    desired_index = date[desired_date]\n",
    "\n",
    "    x, y = createData(symbol, value, data)\n",
    "    desired_data = []\n",
    "    for row in x:\n",
    "        if row[0] == desired_index:\n",
    "            desired_data = row\n",
    "            break\n",
    "\n",
    "    # Normalize the input features using the same MinMaxScaler used during training\n",
    "    desired_data = np.array(desired_data).reshape(-1, 1)\n",
    "    desired_data_normalized = MLRscaler.transform(desired_data)  # Convert to a 2D array\n",
    "    desired_data_normalized = desired_data_normalized.reshape(1, -1)\n",
    "\n",
    "    # Use the trained linear regression model to make predictions\n",
    "    predicted_normalized_value = model.predict(desired_data_normalized)\n",
    "\n",
    "    predicted_original_scale = MLRscaler.inverse_transform(predicted_normalized_value)\n",
    "\n",
    "    return predicted_original_scale\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_using_linear_regression(company_to_predict, value_to_predict, date_to_predict):\n",
    "    linear_regression_model = linear_regression(company_to_predict, value_to_predict, data)\n",
    "    print(mlr_predict(company_to_predict, date_to_predict, value_to_predict, linear_regression_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTMscaler = MinMaxScaler()\n",
    "\n",
    "def lstm(symbol, value, data):\n",
    "    xData, yData = createData(symbol, value, data)\n",
    "    \n",
    "    # training data 0.8, testing data 0.2\n",
    "    split_range = lambda x: int(len(x) * 0.8)\n",
    "    x_train, x_test = xData[:split_range(xData)], xData[split_range(xData):]\n",
    "    y_train, y_test = yData[:split_range(yData)], yData[split_range(yData):]\n",
    "    \n",
    "    x_train = LSTMscaler.fit_transform(x_train)\n",
    "    x_test = LSTMscaler.transform(x_test)\n",
    "\n",
    "    # Reshape the y data for the scaler\n",
    "    y_train = np.array(y_train).reshape(-1, 1)\n",
    "    y_test = np.array(y_test).reshape(-1, 1)\n",
    "\n",
    "    # Normalize the output data (y_train and y_test)\n",
    "    y_train = LSTMscaler.fit_transform(y_train)\n",
    "    y_test = LSTMscaler.transform(y_test)\n",
    "    \n",
    "    # Create a model\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.LSTM(100, return_sequences=True, input_shape=(x_train.shape[1], 1)))\n",
    "    model.add(keras.layers.LSTM(100, return_sequences=False))\n",
    "    model.add(keras.layers.Dense(25))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    model.summary()    \n",
    "\n",
    "    # Compile the model and get initial mean_squared_error before training\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    test_loss = model.evaluate(x_test, y_test)\n",
    "    print(f'Test loss before training: {test_loss}')\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(x_train, y_train, batch_size= 2, epochs=5, validation_data=(x_test, y_test))\n",
    "\n",
    "    # Get mean_squared_error after training\n",
    "    test_loss = model.evaluate(x_test, y_test)\n",
    "    print(f'Test loss after training: {test_loss}')\n",
    "\n",
    "    y_pred = model.predict(x_test)  # Make predictions on the test data\n",
    "\n",
    "    \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    print(f'Root Mean Squared Error (RMSE): {rmse}')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_predict(symbol, desired_date, value, model):\n",
    "    desired_index = date[desired_date]\n",
    "\n",
    "    x, y = createData(symbol, value, data)\n",
    "    desired_data = []\n",
    "    for row in x:\n",
    "        if row[0] == desired_index:\n",
    "            desired_data = row\n",
    "            break\n",
    "\n",
    "    # Normalize the input features using the same MinMaxScaler used during training\n",
    "    desired_data = np.array(desired_data).reshape(-1, 1)\n",
    "    desired_data_normalized = LSTMscaler.transform(desired_data)  # Convert to a 2D array\n",
    "    \n",
    "    # Use the trained LSTM model to make predictions\n",
    "    predicted_normalized_value = model.predict(desired_data_normalized)\n",
    "\n",
    "    predicted_original_scale = LSTMscaler.inverse_transform(predicted_normalized_value)\n",
    "\n",
    "    return predicted_original_scale\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_using_lstm(company_to_predict, value_to_predict, date_to_predict):\n",
    "    lstm_model = lstm(company_to_predict, 'Open', data)\n",
    "    print(lstm_predict(company_to_predict, date_to_predict, value_to_predict, lstm_model))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
